AWS Important Notes - 
1) IAM - 
2) EC2 Instance - AMI(OS) + security group + Instance Size(CPU + RAM) + Storage + EC2 User data.
3) EBS - Elastic Block Store(Volume) is a network drive you can attach to your instances while they run 
   and instances to persist data, even after their termination, they are bound to a specific availability 
   zone and could detached from one EC2 and attached another EC2, ESB can be attached to multiple instances. 
   Make an Amazon EBS volume available for use - https://docs.aws.amazon.com/ebs/latest/userguide/ebs-using-volumes.html 
   .One could take snapshots and copy it across the AZ or region. 
4) AMI - Amazon Machine Image - They are a customization of an EC2 instance, are built for a specific region and can be copied to other regions.
5) EC2 Instance Store - Instead of EBS volume which are network drive with good but limited performance, 
   so for that we use EC2 IS which is a high-performance disk. If stopped losses the data and good for 
   temporary content, good for buffer, cache and scratch data.
6) EFS - Elastic File System is a managed network file system that can be mounted on 100s of EC2. 
   Work with Linux EC2 instances in multi-AZ via EFS Mount Target. There is no capacity planning, 
   highly available and scalable but expensive.
   - EFS Infrequent Access is a storage class that is cost optimized for flies not accessed 
     every day based on their usage last time (60 days) enabled with lifecycle policy.
7) Autoscaling and Load Balancing
  - basics -
      Scalability and Elasticity - 
        1st Is ability to scale up or scale out.
        2nd Is that there will be auto-scaling for system to scale based on load.
8) ELB - Elastic Load Balancer - Comes under EC2 and it is managed load balancer.
   - 4 Kinds of Load Balancers in AWS -
     1) Application LB (HTTP/HTTPS/RPC) - Layer 7, routing features, static DNS(URL). 
     2) Network LB (ultra-high performance, allows for TCP and UDP) - Layer 4 - Static IP via Elastic IP.
     3) Gateway LB (GENEVE Protocol) - Layer 3 - this protocol on IP Packets, routes traffic to Firewalls set by us, Intrusion detection. 
     4) Classic LB - Layer 4 & 7 (retired).
9) S3 - Backup and storage, disaster recovery, hybrid cloud storage, media hosting, 
   application host, archive, data lakes, software delivery, static website. Allows people to store 
   objects(files) in buckets(directories). Buckets are defined at region level and must have globally 
   unique name. Objects have a key which is the full path and composed of prefix + object name. 
   They could be long names that have slashes. Values of Object are the content of body and if < 5GB, 
   must use multi-part upload. Metadata (list of text key / value pairs - system or user metadata). 
   Tags are also used (Unicode key / value pair - up to 10)- useful for security / lifecycle. 
   Version based Id is there too if enabled. IAM Policies are used for User-Based access 
   and Bucket Policies is a bucket wide from S3 console - allows cross accounts. 
   Object and Bucket Access Control List - less common. Bucket Policy to grant public access to 
   bucket and force objects to be encrypted at upload. Static Web Hosting can be enabled and S3 
   bucket's objects can be accessed. We can do versioning at the bucket level, same key overwrites 
   will change the version so we version the buckets instead, also allows easy roll back. 
   Replication requires versioning enabled and we can do 2 types - CRR(Cross region replication) 
   and SRR(Same region replication), copying is async. and IAM permissions must be given to S3.
   Storage Class - Standard, Standard - Infrequent Access, One Zone - IA, Glacier(Instant, Flexible, 
   Deep Archive), Integlligent Tiering. Lifecycle rule lets us automate things like changing between 
   various storage class. Choose the actions we want to perform by the rule like moving between storage class or delete. 
   Choose transitions to move current versions of objects between storage 
   classes based on your use case scenario and performance access requirements. 
   These transitions start from when the objects are created and are consecutively applied.
   S3 Encryption - Server-side Encryption(after uploading) and Client-side Encryption(before upload).
   IAM Access Analyzer for S3 for only few to have access to our buckets and evaluates the bucket Policies,
   ACLs, Access point Policies.
10) Snow Family - Highly secure, portable devices to collect and process data at edge,
    and migrate data into/out of AWS. Has 3 services - snowcone, snowball edge, snowmobile.
    Edge Computing - 
    1) Snowcone & Snowcone SSD (smaller) - 2 CPUs, 4 GB of memory, wired or wireless access.
    2) Snowball Edge - Compute Optimized - 104 vCPU, 416 GiB RAM, Optional GPU, 28 TB NVMe. Storage Clustering. 
    3) Snowball Edge - Storage Optimized - Up to 40 vCPUs, 80 GiB of RAM, 80 TB storage, 
       Up to to 104 vCPUs, 416 GiB of RAM, 210 TB NVMe storage.
11) AWS Storage Cloud Native Options - 
    Block - Amazon EBS, EC2 Instance Store
    File - EFS
    Object - S3, Glacier
12) Storage Gateway - Hybrid storage service to allow on-premises to seamlessly use AWS Cloud
    Types - 
    1) File Gateway
    2) Volume Gateway
    3) Tape Gateway
13) RDS - Reational Database Service, it's a managed DB service for DB use SQL.
    Automated provisioning, OS patching, continuous backups and retore to
    specific timesatamp, multi-AZ setup for disaster recovery, maintenance windows, 
    monitoring, scaling capability, storage backed by EBS, replica reads, no SSH into instances.
    We create RDS and take its snapshot where we can restore, copy, delete, share, migrate.
    RDS Sol. Arch. - Elastics Load Balancer-->Multi-EC2 instances-->RDS.
    RDS Deployments - Read Replicas -> Helps in rapid scaling, can create upto 15 replicas, data written to only main DB.
                      Multi-AZ -> Fail-over in case of AZ outage, data read/write to main DB only.
                      Multi-Region -> Combine the above two over mu regions.
14) Aurora - PSQL, MySQL are both supported as Aurora DB and it is cloud optimized and claims 5x performance
    imporvement over MySQL on RDS and 3x over PSQL on RDS. Aurora storage automatically
    grows in increments of 10GB, up to 128TB. Not in free tier. There is also Aurora 
    Serverless which has automated database instanisation and auto-scaling based on actual
    usage, no capacity planning needed, least management overhead and pay per second.
15) ElastiCache - Managed Redis, Memchached, caches are in-memory databases with high performance and low latency.
    Helps reduce load off databases  for read intensive operations.  
    Arch. - ELB - EC2 with ASG |--> ElastiCache
                               |
                               |--> RDS  
16) Dynamo DB - Fully managed high availability and replication. AWS's NoSQL Key-Value database.
    Scales massively for serverless workloads. Single Digit milisecond latency with 100s TB of storage.
    Integrated with IAM. Has Standard and IA Table class. Has DynamoDB Accelerator which is a 
    in-memory database for DynamoDB and improves 10x from milisecond latency to microsecond.
    Here the table, the primary key is the hash of partition key used for retrieval of items from the 
    table and allocate data across hosts for scalability and availability, and sort key which for 
    optional that allows us to sort or search among all items sharing the same primary key.
    Global Tables - make it accessible with low latency in multi-regions.
17) RedShift - It's OLAP - online analytical Processing. It serves as data warehousing.
    Load data every hour, columnar instead of row based storage, 10x better performance than others,
    massively parallel query execution. Has SQL interface for performing queries and BI tools 
    such as AWS QuickSight or Tableau integrate with it. Has a serverless option and offers
    automatic provisions and scales data warehouse underlying capacity. Run analytics workloads
    without managing data warehouse infra.
18) EMR - Elastic MapReduce helps create Hadoop Clusters for Big data and can be made up of 100s
    of EC2 instances and also supports Apache Spark, HBase, Presto, Flink. It takes care of provisioning
    and congiguration and auto scaling is there too.
19) Athena - Serverlesss query service for performing analytics aganist S3 objects and uses
    standard SQL query and supports CSV, JSON, ORC, Avro and Parquet. Cost - $5.00 per TB of data
    scanned and use compressed or columnar data for cost-savings. 
20) QuickSight - Serverless machine learning-powered business intelligence service 
    to create interactive dashboards.
21) DocumentDB - AWS implementation of MongoDB, fully managed, highly available with replication across 3 AZ.
    It automatically increments by 10 GB.
22) Neptune - Managed graph database and used for various social networks. And highly available across 3 AZ,
    with up to 15 read replicas. Has milisecond latency.
23) TimeStream - Fully managed fast, scalable, serverless time series database. Automatically scale up/down
    to adjust capacity. Store and analyze trillions of events per day, 1000s times faster and 1/10th the cost
    of RD. Built-in time series analytics functions. 
24) QLDB - Quantum Ledger DB and used for recording financial transactions and fully managed serverless, 
    immutable system and used for review history, replication across 3 AZ.
25) Managed Bkockchain - Is a managed service to join public blockchain networks or create your own scalable 
    private network. Compatible with Hyperledger Fabric and Ethereum.
26) Glue - Managed ETL service, useful to prepare and transform data for analytics, fully serverless.
27) Database Migration Service - Source DB -> EC2 running DMS -> Target DB, it is self healing, resilient. 
    Homogenous and heterogenous migrations can be performed.







Note:- 
Edge computing is a distributed computing framework that brings enterprise applications 
closer to data sources such as IoT devices or local edge servers. 
This proximity to data at its source can deliver strong business 
benefits, including faster insights, improved response times and better 
bandwidth availability.




Quota resolution - 
The instance launch failed because the instance type you selected requires more vCPU capacity than your current vCPU limit allows for that instance type's bucket.
Resolution
Amazon Q recommends the following steps to resolve your error


If you don't have permissions to do the following changes, contact your AWS Administrator.

Open the AWS Service Quotas console
In the navigation pane, choose 'EC2 Instance vCPU limits'
Select the region 'us-west-2'
Locate the quota named 'Running On-Demand Standard (A, C, D, H, I, M, R, T, Z) Instance Family vCPUs'
Click 'Request quota increase' for this quota
Follow the guided workflow to request a vCPU quota increase for the instance family you need
Once the quota increase is approved, retry launching your EC2 instance.


Here are the next steps after requesting a quota increase for Amazon EC2:

Wait for AWS to review and process your quota increase request. This typically takes 1-2 business days.

You can check the status of your quota increase request in the AWS Service Quotas console. The status will show as "Pending" until it is approved or denied.

Once your request is approved, the new quota limit will be applied to your account. You can then launch EC2 instances up to the new quota limit.

If your request is denied, you can try providing additional justification for the quota increase or consider launching instances in a different AWS Region that has available capacity.

For the most up-to-date information on EC2 quotas and the request process, I recommend referring to the AWS documentation:

AWS Service Quotas documentation: https://docs.aws.amazon.com/servicequotas/latest/userguide/intro.html
How to request a vCPU service quota increase for Amazon EC2: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-resource-limits.html#request-increase









