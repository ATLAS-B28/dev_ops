AWS Important Notes - 
1) IAM - 
2) EC2 Instance - AMI(OS) + security group + Instance Size(CPU + RAM) + Storage + EC2 User data.
3) EBS - Elastic Block Store(Volume) is a network drive you can attach to your instances while they run 
   and instances to persist data, even after their termination, they are bound to a specific availability 
   zone and could detached from one EC2 and attached another EC2, ESB can be attached to multiple instances. 
   Make an Amazon EBS volume available for use - https://docs.aws.amazon.com/ebs/latest/userguide/ebs-using-volumes.html 
   .One could take snapshots and copy it across the AZ or region. 
4) AMI - Amazon Machine Image - They are a customization of an EC2 instance, are built for a specific region and can be copied to other regions.
5) EC2 Instance Store - Instead of EBS volume which are network drive with good but limited performance, 
   so for that we use EC2 IS which is a high-performance disk. If stopped losses the data and good for 
   temporary content, good for buffer, cache and scratch data.
6) EFS - Elastic File System is a managed network file system that can be mounted on 100s of EC2. 
   Work with Linux EC2 instances in multi-AZ via EFS Mount Target. There is no capacity planning, 
   highly available and scalable but expensive.
   - EFS Infrequent Access is a storage class that is cost optimized for flies not accessed 
     every day based on their usage last time (60 days) enabled with lifecycle policy.
7) Autoscaling and Load Balancing
  - basics -
      Scalability and Elasticity - 
        1st Is ability to scale up or scale out.
        2nd Is that there will be auto-scaling for system to scale based on load.
8) ELB - Elastic Load Balancer - Comes under EC2 and it is managed load balancer.
   - 4 Kinds of Load Balancers in AWS -
     1) Application LB (HTTP/HTTPS/RPC) - Layer 7, routing features, static DNS(URL). 
     2) Network LB (ultra-high performance, allows for TCP and UDP) - Layer 4 - Static IP via Elastic IP.
     3) Gateway LB (GENEVE Protocol) - Layer 3 - this protocol on IP Packets, routes traffic to Firewalls set by us, Intrusion detection. 
     4) Classic LB - Layer 4 & 7 (retired).
9) S3 - Backup and storage, disaster recovery, hybrid cloud storage, media hosting, 
   application host, archive, data lakes, software delivery, static website. Allows people to store 
   objects(files) in buckets(directories). Buckets are defined at region level and must have globally 
   unique name. Objects have a key which is the full path and composed of prefix + object name. 
   They could be long names that have slashes. Values of Object are the content of body and if < 5GB, 
   must use multi-part upload. Metadata (list of text key / value pairs - system or user metadata). 
   Tags are also used (Unicode key / value pair - up to 10)- useful for security / lifecycle. 
   Version based Id is there too if enabled. IAM Policies are used for User-Based access 
   and Bucket Policies is a bucket wide from S3 console - allows cross accounts. 
   Object and Bucket Access Control List - less common. Bucket Policy to grant public access to 
   bucket and force objects to be encrypted at upload. Static Web Hosting can be enabled and S3 
   bucket's objects can be accessed. We can do versioning at the bucket level, same key overwrites 
   will change the version so we version the buckets instead, also allows easy roll back. 
   Replication requires versioning enabled and we can do 2 types - CRR(Cross region replication) 
   and SRR(Same region replication), copying is async. and IAM permissions must be given to S3.
   Storage Class - Standard, Standard - Infrequent Access, One Zone - IA, Glacier(Instant, Flexible, 
   Deep Archive), Integlligent Tiering. Lifecycle rule lets us automate things like changing between 
   various storage class. Choose the actions we want to perform by the rule like moving between storage class or delete. 
   Choose transitions to move current versions of objects between storage 
   classes based on your use case scenario and performance access requirements. 
   These transitions start from when the objects are created and are consecutively applied.
   S3 Encryption - Server-side Encryption(after uploading) and Client-side Encryption(before upload).
   IAM Access Analyzer for S3 for only few to have access to our buckets and evaluates the bucket Policies,
   ACLs, Access point Policies.
10) Snow Family - Highly secure, portable devices to collect and process data at edge,
    and migrate data into/out of AWS. Has 3 services - snowcone, snowball edge, snowmobile.
    Edge Computing - 
    1) Snowcone & Snowcone SSD (smaller) - 2 CPUs, 4 GB of memory, wired or wireless access.
    2) Snowball Edge - Compute Optimized - 104 vCPU, 416 GiB RAM, Optional GPU, 28 TB NVMe. Storage Clustering. 
    3) Snowball Edge - Storage Optimized - Up to 40 vCPUs, 80 GiB of RAM, 80 TB storage, 
       Up to to 104 vCPUs, 416 GiB of RAM, 210 TB NVMe storage.
11) AWS Storage Cloud Native Options - 
    Block - Amazon EBS, EC2 Instance Store
    File - EFS
    Object - S3, Glacier
12) Storage Gateway - Hybrid storage service to allow on-premises to seamlessly use AWS Cloud
    Types - 
    1) File Gateway
    2) Volume Gateway
    3) Tape Gateway
13) RDS - Reational Database Service, it's a managed DB service for DB use SQL.
    Automated provisioning, OS patching, continuous backups and retore to
    specific timesatamp, multi-AZ setup for disaster recovery, maintenance windows, 
    monitoring, scaling capability, storage backed by EBS, replica reads, no SSH into instances.
    We create RDS and take its snapshot where we can restore, copy, delete, share, migrate.
    RDS Sol. Arch. - Elastics Load Balancer-->Multi-EC2 instances-->RDS.
    RDS Deployments - Read Replicas -> Helps in rapid scaling, can create upto 15 replicas, data written to only main DB.
                      Multi-AZ -> Fail-over in case of AZ outage, data read/write to main DB only.
                      Multi-Region -> Combine the above two over mu regions.
14) Aurora - PSQL, MySQL are both supported as Aurora DB and it is cloud optimized and claims 5x performance
    imporvement over MySQL on RDS and 3x over PSQL on RDS. Aurora storage automatically
    grows in increments of 10GB, up to 128TB. Not in free tier. There is also Aurora 
    Serverless which has automated database instanisation and auto-scaling based on actual
    usage, no capacity planning needed, least management overhead and pay per second.
15) ElastiCache - Managed Redis, Memchached, caches are in-memory databases with high performance and low latency.
    Helps reduce load off databases  for read intensive operations.  
    Arch. - ELB - EC2 with ASG |--> ElastiCache
                               |
                               |--> RDS  
16) Dynamo DB - Fully managed high availability and replication. AWS's NoSQL Key-Value database.
    Scales massively for serverless workloads. Single Digit milisecond latency with 100s TB of storage.
    Integrated with IAM. Has Standard and IA Table class. Has DynamoDB Accelerator which is a 
    in-memory database for DynamoDB and improves 10x from milisecond latency to microsecond.
    Here the table, the primary key is the hash of partition key used for retrieval of items from the 
    table and allocate data across hosts for scalability and availability, and sort key which for 
    optional that allows us to sort or search among all items sharing the same primary key.
    Global Tables - make it accessible with low latency in multi-regions.
17) RedShift - It's OLAP - online analytical Processing. It serves as data warehousing.
    Load data every hour, columnar instead of row based storage, 10x better performance than others,
    massively parallel query execution. Has SQL interface for performing queries and BI tools 
    such as AWS QuickSight or Tableau integrate with it. Has a serverless option and offers
    automatic provisions and scales data warehouse underlying capacity. Run analytics workloads
    without managing data warehouse infra.
18) EMR - Elastic MapReduce helps create Hadoop Clusters for Big data and can be made up of 100s
    of EC2 instances and also supports Apache Spark, HBase, Presto, Flink. It takes care of provisioning
    and congiguration and auto scaling is there too.
19) Athena - Serverlesss query service for performing analytics aganist S3 objects and uses
    standard SQL query and supports CSV, JSON, ORC, Avro and Parquet. Cost - $5.00 per TB of data
    scanned and use compressed or columnar data for cost-savings. 
20) QuickSight - Serverless machine learning-powered business intelligence service 
    to create interactive dashboards.
21) DocumentDB - AWS implementation of MongoDB, fully managed, highly available with replication across 3 AZ.
    It automatically increments by 10 GB.
22) Neptune - Managed graph database and used for various social networks. And highly available across 3 AZ,
    with up to 15 read replicas. Has milisecond latency.
23) TimeStream - Fully managed fast, scalable, serverless time series database. Automatically scale up/down
    to adjust capacity. Store and analyze trillions of events per day, 1000s times faster and 1/10th the cost
    of RD. Built-in time series analytics functions. 
24) QLDB - Quantum Ledger DB and used for recording financial transactions and fully managed serverless, 
    immutable system and used for review history, replication across 3 AZ.
25) Managed Bkockchain - Is a managed service to join public blockchain networks or create your own scalable 
    private network. Compatible with Hyperledger Fabric and Ethereum.
26) Glue - Managed ETL service, useful to prepare and transform data for analytics, fully serverless.
27) Database Migration Service - Source DB -> EC2 running DMS -> Target DB, it is self healing, resilient. 
    Homogenous and heterogenous migrations can be performed.
28) ECS - Elastic Container Service and launch containers on AWS, we need to provide provision and maintain
    the infra and AWS takes care of starting/stopping containers. Integrates with application
    load balancer. 
29) Fargate - Launch docker containers on AWS and then provision is maintained AWS. It is serverless offering and AWS just runs 
    containers for us based on CPU/RAM we need. 
30) Lambda - No servers to manage and limited in execution time and runs on demand. Scaling is automated.
    Easy pricing, pay per request and compute time. Integrated with other services and event-driven
    and works with other langauges, easy monitoring via CloudWatch and easy to get resources per functions up to 10GB of RAM.
    It has a container image. To create we can do it from scratch, use blueprint or container image is used. Lambda also creates 
    IAM role for our function we can use existing role or use templates. We can create tests where a new event and it is in JSON format.
    We can add trigger, layers and destination. We get priced per request(call) and compute time(duration)
31) API Gateway - Fully managed service for developers to easily createm publish, maintain, monitor, and secure APIs. Serverless
    and scalable, supporting RESTful and Websocket APIs. Supports the following - 
    a) security
    b) auth
    c) API throttling
    d) monitoring
    e) API keys
32) Batch - Fully managed batch processing at any scale, efficiently run 100,000s of computing batch jobs.
    It automatically launch EC2 instances. It provisions the right amount of memory and compute. We only 
    schedule and write the process and AWS Batch does the rest. The jobs are docker image on EC2 instance.
34) Lightsail - It is virtual, storage, databases and networking, low and predictable pricing.
    Simpler alternative to using EC2, RDS, ELB, EBS, Route 53. But has less high availability, 
    Limited AWS Integrations.
35) CloudFormation - Declartive way of outlining your AWS infra, for any supported resources.
    Creates the resources defined in the defined order and with exact configuration.
    Benefits -
     1)  IaC - No resource are manually created.
     2) Cost - Using identifiers in the stack with tags we know what resource cost how much.
     3) Productivity - Ability to destroy and re-create infra on the cloud on the fly. Automated
        generation of Diagram form the templates, Declarative programming.
36) CDK - Define cloud infra using familiar languages. The code is then compiled into CF format.
    Allows to deploy infra and application runtime at the same time. Which is great for Lambda functions
    and Docker containers in ECS and EKS.
37) BeanStalk - It is a developer centric view of deploying an application on AWS.
    It is a PaaS. We have full control of configuration as it is a managed service so most of the configuration
    can be automatically created along with load balancing and autoscaling, provisioning and monitoring.
    While creating a new application we have 2 options - web app or worker service. Also we have autogenerated or custom
    domain names. We then can choose the platform and upload code for that platform or use sample code. And we get presets
    , which are nothing but starting configuration of the instances. Using IAM and EC2 instance profiles are tied to an IAM 
    policy which is used for service access configuration. We can get new service role and custom make EC2 instance profiles.
38) CodeDeploy - To deploy our applications automatically and it works with EC2 and On-Prem Servers as well as Hybrid.
    We provide servers / instances configured before hand with CodeDeploy Agent.
39) CodeCommit - Before pushing the application code to the servers, it is stored in repository, it is competing service 
    to GITHUB.
40) CodeBuild - Building the code in the cloud. Here we run test, compile and produce packages and pull it from the CodeCommit.
    It is fully managed, severless, pay-as-you-go, secure, continuously scalable and highly available.
41) CodePipeline - Orchestrate the different steps to have the code automatically pushed to production.
    It is fully managed, compatible with CodeCommit, CodeBuild, CodeDeploy, Elastic BeanStalk, CloudFormation, GITHUB, 3rd-party
    services and custom plugins.
42) CodeArtifact - Software packages depend on one other to be built and create new ones, so to store and retrive them is called
    artifact management which is what this service does, and it is fully managed, scalable, secure, cost-effective and works with 
    dependencies management tools like NPM, Maven, NUGET, Gradle, PiP, Yarn, etc.
43) CodeStar
44) Cloud9 - It is a cloud based IDE for writing, running, debugging code.
45) System Manager - Helps manage EC2 and On-Premises systems at scale. Another hybrid AWS service, get operational insights about state of your infra.
    Suite of 10+ produts. Most Important Features - 
      a) Patching automation for enchanced compliance.
      b) Run commands across an fleet of servers.
      c) Store parameter configuration with SSM Parameter Store.
    SSM Session Manager - No SSH keys, access or hots nedded. No port 22 for better security. SSM Agent is EC2 instance, supports Linux and other OSs.
    Send session log data to S3 or CloudWatch logs.
    System Manager Param Store - secure storage for config and secerts, contorl access permissions using IAM.  



Note:- 
Edge computing is a distributed computing framework that brings enterprise applications 
closer to data sources such as IoT devices or local edge servers. 
This proximity to data at its source can deliver strong business 
benefits, including faster insights, improved response times and better 
bandwidth availability.




Quota resolution - 
The instance launch failed because the instance type you selected requires more vCPU capacity than your current vCPU limit allows for that instance type's bucket.
Resolution
Amazon Q recommends the following steps to resolve your error


If you don't have permissions to do the following changes, contact your AWS Administrator.

Open the AWS Service Quotas console
In the navigation pane, choose 'EC2 Instance vCPU limits'
Select the region 'us-west-2'
Locate the quota named 'Running On-Demand Standard (A, C, D, H, I, M, R, T, Z) Instance Family vCPUs'
Click 'Request quota increase' for this quota
Follow the guided workflow to request a vCPU quota increase for the instance family you need
Once the quota increase is approved, retry launching your EC2 instance.


Here are the next steps after requesting a quota increase for Amazon EC2:

Wait for AWS to review and process your quota increase request. This typically takes 1-2 business days.

You can check the status of your quota increase request in the AWS Service Quotas console. The status will show as "Pending" until it is approved or denied.

Once your request is approved, the new quota limit will be applied to your account. You can then launch EC2 instances up to the new quota limit.

If your request is denied, you can try providing additional justification for the quota increase or consider launching instances in a different AWS Region that has available capacity.

For the most up-to-date information on EC2 quotas and the request process, I recommend referring to the AWS documentation:

AWS Service Quotas documentation: https://docs.aws.amazon.com/servicequotas/latest/userguide/intro.html
How to request a vCPU service quota increase for Amazon EC2: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-resource-limits.html#request-increase









